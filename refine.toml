# Example configuration file for Refine Vibe Code
# Copy this file to your project root or use --config option

[scan]
# File patterns to include in scanning
include_patterns = [
    "*.py",
    "*requirements*.txt",
]

# File patterns to exclude from scanning
exclude_patterns = [
    "__pycache__/",
    "*.pyc",
    "node_modules/",
    ".git/",
    ".venv/",
    ".env/",
    "build/",
    "dist/"
]

[checkers]
# List of enabled checkers
enabled = [
    # Security Issues
    "hardcoded_secrets",
    "sql_injection",
    "dangerous_ai_logic",

    # Code Quality
    "vibe_naming",
    "comment_quality",
    "edge_cases",

    # Package & Dependencies
    "package_check",
    "dependency_validation",

    # Best Practices
    "boilerplate",
]

# Only run classical (AST-based) checkers
classical_only = false

# Only run LLM-based checkers
llm_only = false

[chunking]
# Maximum lines per chunk (larger = fewer API calls, faster scans)
max_chunk_lines = 500

# Process chunks in parallel (significant speedup for large files)
parallel_chunks = true

# Maximum parallel API requests (4 is a good balance for rate limits)
max_parallel_requests = 10

# Use AST-based boundaries (function/class) instead of line counts
use_ast_boundaries = true

# Combine small files into single chunks to reduce API requests
stack_small_files = true

# Stack files if total is under this fraction of max_chunk_lines (0.0-1.0)
stack_threshold = 0.5

[llm]
# LLM provider (openai, google, claude)
provider = "google"

# Model name to use (see examples below for different providers)
model = "gemini-2.0-flash-exp"

# API key (can also be set via environment variables: OPENAI_API_KEY, GOOGLE_API_KEY, ANTHROPIC_API_KEY)
api_key = ""

# Temperature for responses (0.0 = deterministic, 2.0 = creative)
temperature = 0.1

# Maximum tokens in response
max_tokens = 100000

# Request timeout in seconds
timeout = 60

# LLM Integration Examples:
#
# OpenAI Models:
# provider = "openai"
# model = "gpt-4o"                    # Latest GPT-4 optimized model, great for code analysis
# model = "gpt-4o-mini"               # Cost-effective GPT-4 model, fast and reliable
# model = "gpt-4-turbo"               # Previous generation, good balance of speed/cost
# model = "gpt-3.5-turbo"             # Fastest and cheapest, good for simple checks
#
# Google Gemini Models:
# provider = "google"
# model = "gemini-2.0-flash-exp"      # Latest experimental model, most advanced
# model = "gemini-1.5-pro"            # Stable production model, good performance
# model = "gemini-1.5-flash"          # Fast and cost-effective model
#
# Anthropic Claude Models:
# provider = "claude"
# model = "claude-3-5-sonnet-20241022" # Latest Claude model, excellent for code analysis
# model = "claude-3-5-haiku-20241022"  # Fast and cost-effective model

[output]
# Output format (rich, json, plain)
format = "rich"

# Enable verbose output
verbose = false

# Show suggested fixes
show_fixes = true

# Enable colored output
color = true
