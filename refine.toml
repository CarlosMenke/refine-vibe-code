# Example configuration file for Refine Vibe Code
# Copy this file to your project root or use --config option

[scan]
# File patterns to include in scanning
include_patterns = [
    "*.py",
    "*requirements*.txt",
]

# File patterns to exclude from scanning
exclude_patterns = [
    "__pycache__/",
    "*.pyc",
    "node_modules/",
    ".git/",
    ".venv/",
    ".env/",
    "build/",
    "dist/"
]

# Maximum file size to scan in bytes (1MB)
max_file_size = 1048576

# Maximum number of files to scan
max_files = 1000

[llm]
# LLM provider to use (openai, google)

# Base URL for API (optional, for custom endpoints)
# base_url = "https://api.openai.com/v1"

# === OpenAI Provider Configuration ===
# provider = "openai"
# model = "gpt-4o-mini"
# api_key = "sk-your-openai-api-key-here"

# === OpenAI Provider Configuration ===
#provider = "openai"
# model = "gpt-4o-mini"
# api_key = "sk-your-openai-api-key-here"  # Uncomment and set your API key here

# === Google Gemini Provider Configuration ===
provider = "google"
model = "gemini-2.0-flash-exp"
api_key = "AQ.Ab8RN6KJZnkDbofE5cRd-3DZJYcmSleHvg-8N7do1FXdzfQ-8g"

# Maximum tokens for LLM responses
max_tokens = 1000

# Timeout for LLM requests in seconds
timeout = 30

[checkers]
# List of enabled checkers
enabled = [
    "package_check",
    "boilerplate",
    "hardcoded_secrets",
    "dependency_validation",
    "edge_cases",
    "naming_vibe",
    "comment_quality"
]

# Only run classical (AST-based) checkers
classical_only = false

# Only run LLM-based checkers
llm_only = false

[output]
# Output format (rich, json, plain)
format = "rich"

# Enable verbose output
verbose = false

# Show suggested fixes
show_fixes = true

# Enable colored output
color = true
